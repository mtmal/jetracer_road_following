{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c5a1e35",
   "metadata": {},
   "source": [
    "# Teleoperated Interactive Regression\n",
    "\n",
    "In this example we will use gamepad to control the robot in order to capture drive commands with associated images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579966f",
   "metadata": {},
   "source": [
    "### Create gamepad controller and connect it to the robot - based on teleoperation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774eaa15",
   "metadata": {},
   "source": [
    "First we create a gamepad controller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bcff80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb37b20aef144e0c86549880e9ed0fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Controller()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "controller = widgets.Controller(index=0)  # replace with index of your controller\n",
    "\n",
    "display(controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69382979",
   "metadata": {},
   "source": [
    "Next, we create a robot class.\n",
    "\n",
    "> WARNING: This next cell will move the robot if you touch the gamepad controller axes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6118520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetracer.nvidia_racecar import NvidiaRacecar\n",
    "import traitlets\n",
    "\n",
    "car = NvidiaRacecar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c2b839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "car.throttle_gain   = 0.3\n",
    "car.steering_offset = 0.05\n",
    "car.steering        = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fedeba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell tends to fail when called immediately after the previous (tuple index out of range). When it happens - rerun the cell. Ah Python... ;]\n",
    "steering_link  = traitlets.dlink((controller.axes[0], 'value'), (car, 'steering'), transform=lambda x:-x)\n",
    "throttle_link = traitlets.dlink((controller.axes[3], 'value'), (car, 'throttle'), transform=lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25128079",
   "metadata": {},
   "source": [
    "Now we create a camera to provide a feedback. Notice the reduced framerate. We really do not need 65 FPS. Lower framerate is much better from training perspective so that we do not have multiple data that is not distinctively different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35dcaaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e40a2bba3a414293ec2200aeb63286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jetcam.csi_camera import CSICamera\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "\n",
    "camera = CSICamera(width=224, height=224, capture_fps=5)\n",
    "image = camera.read()\n",
    "image_widget = ipywidgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "\n",
    "image_widget.value = bgr8_to_jpeg(image)\n",
    "\n",
    "display(image_widget)\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248ecbe",
   "metadata": {},
   "source": [
    "As we do not like wasting processing power, make sure camera is running only when driving forward. Furthermore, capturing dataset while stationary could result in overfit. Use backward motion to reposition robot where you want.\n",
    "> NOTE: In this case, negative X value is forward motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12fab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_run_link = traitlets.dlink((car, 'throttle'), (camera, 'running'), transform=lambda x: x < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab39f96",
   "metadata": {},
   "source": [
    "This is a good time to test if the robot is moving correctly and camera widget is updating. If needed, adjust steering offset and throttle_gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeba2f5",
   "metadata": {},
   "source": [
    "### Now it is time to create ML part of the example - based on interactive_regression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82549907",
   "metadata": {},
   "source": [
    "Task - copy-paste, only task name and categories are renamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f2c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from xy_dataset_float import XYDataset_Float\n",
    "\n",
    "TASK = 'road_following_teleoperated'\n",
    "\n",
    "CATEGORIES = ['apex_2']\n",
    "\n",
    "DATASETS = ['A', 'B']\n",
    "\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "datasets = {}\n",
    "for name in DATASETS:\n",
    "    datasets[name] = XYDataset_Float(TASK + '_' + name, CATEGORIES, TRANSFORMS, random_hflip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ea6f2",
   "metadata": {},
   "source": [
    "Data Collection - note the modifications compared to interactive_regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d9c738a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8590a81786d942e0b0edf35262dbadfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize active dataset\n",
    "dataset = datasets[DATASETS[0]]\n",
    "\n",
    "# we do not touch the camera class as it has been already initialised and preview widget was created. Besides, we don't really need a preview.\n",
    "\n",
    "# create widgets\n",
    "dataset_widget = ipywidgets.Dropdown(options=DATASETS, description='dataset')\n",
    "category_widget = ipywidgets.Dropdown(options=dataset.categories, description='category')\n",
    "count_widget = ipywidgets.IntText(description='count')\n",
    "\n",
    "# manually update counts at initialization\n",
    "count_widget.value = dataset.get_count(category_widget.value)\n",
    "\n",
    "# sets the active dataset\n",
    "def set_dataset(change):\n",
    "    global dataset\n",
    "    dataset = datasets[change['new']]\n",
    "    count_widget.value = dataset.get_count(category_widget.value)\n",
    "dataset_widget.unobserve_all()\n",
    "dataset_widget.observe(set_dataset, names='value')\n",
    "\n",
    "# update counts when we select a new category\n",
    "def update_counts(change):\n",
    "    count_widget.value = dataset.get_count(change['new'])\n",
    "category_widget.unobserve_all()\n",
    "category_widget.observe(update_counts, names='value')\n",
    "\n",
    "# this function is only called when new image is acquired.\n",
    "def saveData(change):\n",
    "    # get the robot's steering and throttle instead of X-Y pixel coordinates\n",
    "    x = car.steering\n",
    "    y = car.throttle\n",
    "    # save to disk\n",
    "    dataset.save_entry(category_widget.value, camera.value, x, y)\n",
    "    count_widget.value = dataset.get_count(category_widget.value)\n",
    "image_widget.unobserve_all()\n",
    "image_widget.observe(saveData, names = 'value')\n",
    "\n",
    "data_collection_widget = ipywidgets.VBox([\n",
    "    image_widget,\n",
    "    dataset_widget,\n",
    "    category_widget,\n",
    "    count_widget\n",
    "])\n",
    "\n",
    "display(data_collection_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c1d06f",
   "metadata": {},
   "source": [
    "Model - copy-paste from interactive_regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b457e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f396d42727df4f44b95f1431821b70fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='road_following_model_tele.pth', description='model path'), HBox(children=(Button(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "device = torch.device('cuda')\n",
    "output_dim = 2 * len(dataset.categories)  # x, y coordinate for each category\n",
    "\n",
    "# ALEXNET\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "# model.classifier[-1] = torch.nn.Linear(4096, output_dim)\n",
    "\n",
    "# SQUEEZENET \n",
    "# model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "# model.classifier[1] = torch.nn.Conv2d(512, output_dim, kernel_size=1)\n",
    "# model.num_classes = len(dataset.categories)\n",
    "\n",
    "# RESNET 18\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "# RESNET 34\n",
    "# model = torchvision.models.resnet34(pretrained=True)\n",
    "# model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "# DENSENET 121\n",
    "# model = torchvision.models.densenet121(pretrained=True)\n",
    "# model.classifier = torch.nn.Linear(model.num_features, output_dim)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model_save_button = ipywidgets.Button(description='save model')\n",
    "model_load_button = ipywidgets.Button(description='load model')\n",
    "model_path_widget = ipywidgets.Text(description='model path', value='road_following_model_tele.pth')\n",
    "\n",
    "def load_model(c):\n",
    "    model.load_state_dict(torch.load(model_path_widget.value))\n",
    "model_load_button.on_click(load_model)\n",
    "    \n",
    "def save_model(c):\n",
    "    torch.save(model.state_dict(), model_path_widget.value)\n",
    "model_save_button.on_click(save_model)\n",
    "\n",
    "model_widget = ipywidgets.VBox([\n",
    "    model_path_widget,\n",
    "    ipywidgets.HBox([model_load_button, model_save_button])\n",
    "])\n",
    "\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8851e74",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40cc7c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae3e6f3979448c7b96d314841d70ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntText(value=1, description='epochs'), FloatProgress(value=0.0, description='progress', max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "epochs_widget = ipywidgets.IntText(description='epochs', value=1)\n",
    "eval_button = ipywidgets.Button(description='evaluate')\n",
    "train_button = ipywidgets.Button(description='train')\n",
    "loss_widget = ipywidgets.FloatText(description='loss')\n",
    "progress_widget = ipywidgets.FloatProgress(min=0.0, max=1.0, description='progress')\n",
    "\n",
    "def train_eval(is_training):\n",
    "    global BATCH_SIZE, LEARNING_RATE, MOMENTUM, model, dataset, optimizer, eval_button, train_button, accuracy_widget, loss_widget, progress_widget\n",
    "    \n",
    "    try:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        train_button.disabled = True\n",
    "        eval_button.disabled = True\n",
    "        time.sleep(1)\n",
    "\n",
    "        if is_training:\n",
    "            model = model.train()\n",
    "        else:\n",
    "            model = model.eval()\n",
    "\n",
    "        while epochs_widget.value > 0:\n",
    "            i = 0\n",
    "            sum_loss = 0.0\n",
    "            error_count = 0.0\n",
    "            for images, category_idx, xy in iter(train_loader):\n",
    "                # send data to device\n",
    "                images = images.to(device)\n",
    "                xy = xy.to(device)\n",
    "\n",
    "                if is_training:\n",
    "                    # zero gradients of parameters\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # execute model to get outputs\n",
    "                outputs = model(images)\n",
    "\n",
    "                # compute MSE loss over x, y coordinates for associated categories\n",
    "                loss = 0.0\n",
    "                for batch_idx, cat_idx in enumerate(list(category_idx.flatten())):\n",
    "                    loss += torch.mean((outputs[batch_idx][2 * cat_idx:2 * cat_idx+2] - xy[batch_idx])**2)\n",
    "                loss /= len(category_idx)\n",
    "\n",
    "                if is_training:\n",
    "                    # run backpropogation to accumulate gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                    # step optimizer to adjust parameters\n",
    "                    optimizer.step()\n",
    "\n",
    "                # increment progress\n",
    "                count = len(category_idx.flatten())\n",
    "                i += count\n",
    "                sum_loss += float(loss)\n",
    "                progress_widget.value = i / len(dataset)\n",
    "                loss_widget.value = sum_loss / i\n",
    "                \n",
    "            if is_training:\n",
    "                epochs_widget.value = epochs_widget.value - 1\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(e.message, e.args)\n",
    "        pass\n",
    "    model = model.eval()\n",
    "\n",
    "    train_button.disabled = False\n",
    "    eval_button.disabled = False\n",
    "    \n",
    "train_button.on_click(lambda c: train_eval(is_training=True))\n",
    "eval_button.on_click(lambda c: train_eval(is_training=False))\n",
    "    \n",
    "train_eval_widget = ipywidgets.VBox([\n",
    "    epochs_widget,\n",
    "    progress_widget,\n",
    "    loss_widget,\n",
    "    ipywidgets.HBox([train_button, eval_button])\n",
    "])\n",
    "\n",
    "display(train_eval_widget)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
